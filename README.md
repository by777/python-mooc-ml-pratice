# Python机器学习应用

## 无监督学习

利用无标签的数据学习数据的分布或数据与数据之间的关系称作无监督学习。

+ 无监督学习与有监督学习的最大区别在于数据是否有标签
+ 无监督学习最常用的场景是聚类（clustering）和降维（dimension reduction）

### 聚类（Clustering）

聚类就是利用数据的相似性把数据分为多类的过程。评估两个不同样本的相似性，最常使用的方法就是计算两个样本之间的“距离”。

欧氏距离：欧氏空间两点的直线距离
$$
d = \sqrt{\sum_{k=1}^{n} (x_{1k} - x_{2k})^2}
$$

曼哈顿距离：又称为城市街区距离
$$
d = \sum_{k=1}^{n} |x_{1k} - x_{2k}|
$$

马氏距离：数据的协方差距离，与尺度无关

夹角余弦：余弦值越接近于1说明两个向量夹角越接近于0，越相似

###  降维(Dimension Reduction)

降维，就是在保证数据所具有的代表性特性或者分布的情况下，将高维数据转化为低维。

+ 数据的可视化

+ 精简数据

## 监督学习

利用一组带有标签的数据，学习从输入到输出的映射，然后将这种映射关系应用到未知数据上，达到分类或回归的目的。

+ 分类：当输出是离散的，学习任务为分类任务
+ 回归：当输出是连续的，学习任务为回归任务

训练集 / 测试集的划分方法：7：3

### 分类学习

评价标准：

`精确率`：精确率是针对我们预测结果而言的，（以二分类为例）它表示的是预测为正的样本中有多少是真央的样本。那么预测为正就有两种可能了，一种就是把正类预测为正类（TP），另一种就是把负类预测为正类（FP），也就是
$$
P = {TP \over {TP + FP}}
$$
`召回率`：是针对我们原来的样本而言的，它表示样本中正例有多少被正确预测了。那也有两种可能，一种是把原来的正类预测为正类（TP），另一种就是把原来的正类预测为负类（FN），也就是
$$
P = {TP \over {TP + FN}}
$$

sklearn提供的分类函数

+ knn
+ naivebayes
+ svm
+ decision tree
+ neural networks等

### 回归分析

回归：统计学分析数据的方法，目的在于了解两个或多个变量之间是否相关、研究其相关方向与强度，并建立数学模型。回归分析可以帮助人们在了解自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期许。

Sklearn提供的回归函数主要被封装在两个子模块中，分别是：sklearn.linear.model和sklearn.processing

+ 普通线性回归函数（Linear Regression）  
+ 岭回归（Ridge）  
+ Lasso  
+ 非线性回归函数，如多项式回归（PolynomialFeatures）则通过sklearn.preprocessing子模块调用
  
## 强化学习

强化学习激就是程序或者智能体（agent）通过与环境不断的进行交互学习一个从环境到动作的映射，学习的目标就是使累计回报最大化。

换句话说，强化学习是一种试错学习，因其在各种状态（环境）下需要尽量尝试所有可选择的动作，通过环境给出的反馈（即奖励）来判断动作的优劣，最终获得环境和最优动作的映射（即策略）。

### 马尔可夫决策过程（MDP）

马尔可夫决策过程（Markov Decision Process）通常用来描述一个强化学习问题。即一个智能体（agent）根据当前环境的观察采取动作获得环境的反馈，并使环境发生改变的循环过程。

基本元素：

+ s∈S：有限个状态state集合，s表示某个特定状态
  
+ a∈A：有限个动作action集合，a表示某个特定动作
  
+ T(S,a,S') ~ Pr(s' | s, s)：状态转移模型，根据当前状态s和动作a预测下一个状态s，这里的Pr表示从s采取行动a转移到s'的概率
  
+ R(s,a)：表示agent采取某个动作后的即时奖励，它还有R(s,a,s'),R(s)等表现形式
  
+ Policy  Π(s) -> a：根据当前state来产生action，可表现为a = Π(s) 或Π(a|s) = P(a|s)，后者表示某种状态下执行某个动作的概率

### 蒙特卡洛强化学习

在现实的强化学习任务中，环境的转移概率、奖励函数往往很难得知，甚至很难得知环境中有多少状态。若学习算法不再依赖于环境建模，则称为免模型学习，蒙特卡洛强化学习就是其中一种。

蒙特卡洛强化学习使用多次采样，然后求取平均累计奖赏作为期望累计奖赏的近似。

蒙特卡洛直接对状态动作值函数Q(s,a)进行估计，每采样一条轨迹，就根据轨迹中的所有“状态-动作”利用下面的公式来对值函数进行更新。
$$
{Q(s,a)} = {{Q(s,a) * count(s,a) + R} \over {count(s,a) + 1}}
$$
每次采样更新完所有的“状态-动作”对所对应的Q(s,a)，就需要更新采样策略Π。但由于策略可能是确定性的，即一个状态对应一个动作，多次采样可能获得相同的采样轨迹，因此需要借助з贪心策略。

蒙特卡洛强化学习算法需要采样一个完整的轨迹来更新值函数，效率较低，此外该算法没有充分利用强化学习任务的序贯决策结构。

Q-learning算法结合了动态规划与蒙特卡洛方法的思想，使得学习更加高效。

### Q-learning算法

假设对于状态动作对（s,a）基于t次采样估计出其值函数为：

​      
$$
{Q_t^Π(s,a)} = {{\sum^t_{i=1} r_i} \over t * }
$$
在进行t+1次采样后，依据增量更新。

###   深度强化学习（DRL）

+ 传统强化学习：真实环境中的状态数目过多，求解困难。
+ 深度强化学习：通过深度神经网络直接学习环境（或观察）与状态动作值函数Q(s,a)之间的映射关系，简化问题的求解。

### Deep Q Network(DQN)  

将NN与Q-learning结合，利用NN近似模拟函数Q(s,a)，输入是问题的状态（如图像），输出是每个动作对应的Q值，然后依据Q值的大小选择对应的动作。